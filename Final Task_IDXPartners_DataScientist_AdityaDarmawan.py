# -*- coding: utf-8 -*-
"""Muhamad Aditya Darmawn - End-to-end Solution.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1M5n-ZqFaExIRXWK3oJzEffs9S0cOcz_g

# **Credit Risk Analysis**
Muhamad Aditya Darmawan

#### **1. Importing Data & Libraries**
"""

from google.colab import drive
drive.mount("/content/drive")

# Commented out IPython magic to ensure Python compatibility.
# Importing Libraries
import pandas as pd
import numpy as np

from sklearn.model_selection import KFold
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import cross_val_predict
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
from sklearn import svm
from sklearn.pipeline import Pipeline
from sklearn.metrics import recall_score
from scipy import stats
import statsmodels.api as sm
from statsmodels.formula.api import ols
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
import warnings

import itertools

from imblearn.over_sampling import SMOTE

# %matplotlib inline
warnings.filterwarnings('ignore')

# Importing Data
data = pd.read_csv('/content/drive/My Drive/01. Kuliah/Magang/IDX/Final Project/Final Task_IDXPartners_DataScientist_AdityaDarmawan/loan_data_2007_2014.csv')

# Extending Output Line Amount
pd.options.display.max_rows = 10000000
np.set_printoptions(edgeitems = 25, linewidth = 10000000)

"""#### **2. Defining Functions**"""

# Commented out IPython magic to ensure Python compatibility.
# Confusion Matrix Plot
def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    #Add Normalization Option
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    print(cm)

    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')

# Feature Importance Plot
def plot_feature_importances(model,X_train):

    xaxes = list(X_train.columns.values)
    yaxes = list(model.feature_importances_)

    h = dict(zip(xaxes,yaxes))
    s = [(k, h[k]) for k in sorted(h, key=h.get, reverse=False)]
    s = dict(s)
    xaxes = list(s.keys())
    yaxes = list(s.values())

    n_features = X_train.shape[1]
    plt.figure(figsize=(8,8))
    plt.barh(range(n_features), yaxes, align='center')
    plt.yticks(np.arange(n_features), xaxes)
    plt.xlabel("Feature importance")
    plt.ylabel("Feature")

def rocurve(logreg, X_train, X_test, y_train, y_test):

    from sklearn.metrics import roc_curve, auc
    import matplotlib.pyplot as plt
    import seaborn as sns
#     %matplotlib inline

    y_test_score = logreg.decision_function(X_test)
    y_train_score = logreg.decision_function(X_train)

    test_fpr, test_tpr, test_thresholds = roc_curve(y_test, y_test_score)
    train_fpr, train_tpr, train_thresholds = roc_curve(y_train, y_train_score)

    print('Test AUC: {}'.format(auc(test_fpr, test_tpr)))
    print('train AUC: {}'.format(auc(train_fpr, train_tpr)))

    plt.figure(figsize=(10,8))
    lw = 2
    plt.plot(test_fpr, test_tpr, color='darkorange',
             lw=lw, label='Test ROC curve')
    plt.plot(train_fpr, train_tpr, color='blue',
             lw=lw, label='train ROC curve')
    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.yticks([i/20.0 for i in range(21)])
    plt.xticks([i/20.0 for i in range(21)])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver operating characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    plt.show()

# Determine Dimensions to Preserve the Level of Variance
def find_min_pca(X_train, percentage=0.8):
    from sklearn.decomposition import PCA
    inital_components=pd.DataFrame(X_train).shape[1]
    if inital_components<10:
        print('The Data has less than 10 components, no PCA needed')
    else:
        a=int(str(inital_components)[-1])
        tens=int((inital_components-a)/10)
        for i in range(1,tens+1):
            pca = PCA(n_components=(i)*10)
            principalComponents = pca.fit_transform(X_train)
            explained_var= np.sum(pca.explained_variance_ratio_)
            if explained_var>=percentage:
                upper=i
                break
        for j in range((upper-1)*10,(upper)*10):
            pca = PCA(n_components=j)
            principalComponents = pca.fit_transform(X_train)
            explained_var1= np.sum(pca.explained_variance_ratio_)
            if explained_var1>percentage:
                comp=j
                break
    return comp

# Determine FPR and TPR
def perf(y_test, y_pred):
    from sklearn.metrics import confusion_matrix
    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()
    fpr = round(fp  / (fp + tn),2)*100
    tpr = round(tp / (tp+fn),2)*100

    return(print(str(tpr) + '% of the positives are appropriately identified, and ' + str(fpr) + '% of the negatives are appropriately identified.'))

"""#### **3. Exploratory Data Analysis**

**Reading Data Columns**
"""

data.columns

"""**Data Exploration**"""

data.loan_status.value_counts()

# Fully Paid (184739): Loan has been 100% paid off
# Charged Off (42475): Loan which has been stopped for further payments

# The machine will aim for the variable 'loan_status'. We classify the Fully Paid loan as 1 and the Charged Off loan as 0, and the rest will be ignored.

data = data[(data['loan_status'] == 'Fully Paid') | (data['loan_status'] == 'Charged Off')]
data.loan_status.value_counts()

# Converting Loan Status into Binary
data.loan_status[(data['loan_status'] == 'Fully Paid')] = 0
data.loan_status[(data['loan_status'] == 'Charged Off')] = 1

# Printing Loan Status as Float
data.loan_status.astype(float)

# Counting the Amount of Columns
len(data.columns)

# Creating List to Store Categorical and Numerical Features
cat_feat = []
num_feat = []

"""##### **Column 1 - 10**"""

data.columns[1:11]

for col in data.columns[1:11]:
    print(col,'\n',data[col].head(10),'\n')

# loan_amnt: numerical feature
# term: categorical feature
# int_rate: numerical feature
# installment: numerical feature

cat_feat.append('term')
num_feat.append('loan_amnt')
num_feat.append('installment')
num_feat.append('int_rate')

"""##### **Column 11 - 20**"""

data.columns[11:21]

for col in data.columns[11:21]:
    print(col,'\n',data[col].head(10),'\n')

# emp_length: can be numerical feature
# home_ownership: will be converted into categorical feature
# annual_inc: numerical feature
# verification_status: categorical feature

cat_feat.append('home_ownership')
cat_feat.append('verification_status')
num_feat.append('emp_length')
num_feat.append('annual_inc')

"""##### **Column 21 - 30**"""

data.columns[21:31]

for col in data.columns[21:31]:
    print(col,'\n', data[col].head(10),'\n')

# purpose: can be categorical feature
# dti: numerical feature
# delinq_2yrs: numerical feature
# inq_last_6mths: numerical feature

cat_feat.append('purpose')
num_feat.append('dti')
num_feat.append('delinq_2yrs')
num_feat.append('inq_last_6mths')

"""##### **Column 31 - 40**"""

data.columns[31:41]

for col in data.columns[31:41]:
    print(col,'\n',data[col].head(10),'\n')

# open_acc: numerical feature
# pub_rec: will be converted into numerical feature
# revol_bal: numerical feature
# revol_util: numerical feature
# total_acc: numerical feature

num_feat.append('pub_rec')
num_feat.append('open_acc')
num_feat.append('revol_bal')
num_feat.append('revol_util')
num_feat.append('total_acc')

"""##### **Column 41 - 50**"""

data.columns[41:51]

for col in data.columns[41:51]:
    print(col,'\n',data[col].head(10),'\n')

# No Variable is Necessary for the Model

"""##### **Column 51 - 60**"""

data.columns[51:61]

for col in data.columns[51:61]:
    print(col,'\n',data[col].head(10),'\n')

# No Variable is Necessary for the Model

"""##### **Column 61 - 70**"""

data.columns[61:71]

for col in data.columns[61:71]:
    print(col,'\n',data[col].head(10),'\n')

# No Variable is Necessary for the Model

"""##### **Column 71 - 74**"""

data.columns[71:]

for col in data.columns[71:]:
    print(col,'\n',data[col].head(10),'\n')

# No Variable is Necessary for the Model

"""##### **Conclusion**"""

print('We have', len(cat_feat), 'categorical features and', len(num_feat), 'numerical features.')

cat_feat

num_feat

"""**Data Cleaning**"""

# Counting Nulls in the Data Set
data.isna().sum()

# Dropping Nulls in the Categorical and Numerical Feature
data.dropna(subset = ['revol_util', 'last_credit_pull_d'], inplace = True)

"""**Can the Nulls in emp_length be dropped?**"""

data.loan_status[data.emp_length.isna() == True].value_counts()

"""2318 out of 42475 loans (5%) that were charged off occured on rows where information about employment length is absent. The 5% of the rows with loans charged off can be dropped without significant impact."""

data.dropna(subset = ['emp_length'], inplace = True)

data.reset_index(inplace = True)

# Dataframe for Categorical Data
data_categorical = pd.DataFrame()
for col in cat_feat:
    data_categorical = pd.concat([data_categorical, data[col]], axis = 1)

data_categorical.head()

# Dataframe for Numerical Data
data_numerical = pd.DataFrame()
for col in num_feat:
    data_numerical = pd.concat([data_numerical, data[col]], axis = 1)

data_numerical.head()

# Checking Nulls in Numerical Features
data_numerical.isna().sum()

data_numerical.emp_length.unique()

# Converting emp_length to Numerical
mapping_dict = {
    'emp_length': {
        '10+ years': 10,
        '9 years': 9,
        '8 years': 8,
        '7 years': 7,
        '6 years': 6,
        '5 years': 5,
        '4 years': 4,
        '3 years': 3,
        '2 years': 2,
        '1 year': 1,
        '< 1 year': 0
    }
}

data_numerical.replace(mapping_dict, inplace = True)

data_numerical.head()

for col in data_numerical.columns:
    data_numerical[col].astype(float)

data_numerical.describe()

"""**Categorical Data**"""

data_categorical.isna().sum()

data_categorical.info()

for col in data_categorical.columns:
    print(col, '\n', data_categorical[col].unique(), '\n\n')

data_categorical.head()

# Creating Dummy Variables for Scaling
for col in data_categorical.columns:
    dummies = pd.get_dummies(data_categorical[col])
    dummies.drop(dummies.columns[-1], axis = 1, inplace = True)
    data_categorical = pd.concat([data_categorical, dummies], axis = 1)
    data_categorical.drop(col, axis = 1, inplace = True)

data_categorical.head()

for col in data_categorical.columns:
    data_categorical[col].astype(int)

# Scaling

scaler = StandardScaler()
scaled_num = pd.DataFrame(scaler.fit_transform(data_numerical),columns=data_numerical.columns)

X = pd.concat([scaled_num,data_categorical],axis=1)

y = data.loan_status
print(X.shape, y.shape)
X.head()

"""#### **4. SMOT for Class Imbalance**"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 10)
y_test = list(y_test)

X_train.head()

print(y_train.value_counts())
y_train_for_smote = [i for i in y_train]
oversample = SMOTE()
X_train_resampled, y_train_resampled = oversample.fit_resample(X_train, y_train_for_smote)
print(pd.Series(y_train_resampled).value_counts())
y_train_resampled = list(y_train_resampled)
y_train = list(y_train)

"""#### **5. Performance Metric Selection**

The performance evaluation metric of this model will be Recall. TPR also needs to be evaluated to make sure the model doesn't decline too many qualified creditor.

**Logistic Regression**
"""

model = LogisticRegression()
model.fit(X_train_resampled, y_train_resampled)

y_pred = model.predict(X_test)
cnf_matrix = confusion_matrix(y_test, y_pred)
plot_confusion_matrix(cnf_matrix, classes = [0, 1])

perf(y_test, y_pred)

rocurve(model, X_train_resampled, X_test, y_train_resampled, y_test)

"""**Random Forest**"""

model = RandomForestClassifier()
model.fit(X_train_resampled, y_train_resampled)
y_pred = model.predict(X_test)

cnf_matrix = confusion_matrix(y_test, y_pred)
plot_confusion_matrix(cnf_matrix, classes=[0,1])

perf(y_test, y_pred)

plot_feature_importances(model, X_train)

"""**Support Vector Machine**"""

# Finding Amount of Principal Components to Use to Preserve 90% of Variance
min_comps = find_min_pca(X_train_resampled, 0.9)
print(min_comps)

pca = PCA(n_components = min_comps)
principalComponents = pca.fit_transform(X_train_resampled)
print(np.sum(pca.explained_variance_ratio_))

model = Pipeline([('pca', PCA(n_components = min_comps)), ('clf', svm.SVC(random_state = 123))])
model.fit(X_train_resampled, y_train_resampled)
y_pred = model.predict(X_test)

cnf_matrix = confusion_matrix(y_test, y_pred)
plot_confusion_matrix(cnf_matrix, classes = [0, 1])

perf(y_test, y_pred)

"""**XGBoost**"""

import xgboost as xgb
from sklearn.model_selection import GridSearchCV

model = xgb.XGBClassifier()
model.fit(X_train_resampled, y_train_resampled)
training_preds = model.predict(X_train_resampled)
X_test2 = np.array(X_test)
y_pred = model.predict(X_test2)
training_accuracy = accuracy_score(y_train_resampled, training_preds)
val_accuracy = accuracy_score(y_test, y_pred)

print("Training Accuracy: {:.4}%".format(training_accuracy * 100))
print("Validation accuracy: {:.4}%".format(val_accuracy * 100))
perf(y_test,y_pred)

param_grid = {
    "learning_rate": [0.1],
    'max_depth': [6,8],
    'min_child_weight': [7,8,10],
    'subsample': [ 0.7,0.9],
    'n_estimators': [5, 30, 100, 250],
}

grid_clf = GridSearchCV(model, param_grid, scoring='recall', cv=None, n_jobs=1)
grid_clf.fit(X_train_resampled, y_train_resampled)

best_parameters = grid_clf.best_params_

print("Grid Search found the following optimal parameters: ")
for param_name in sorted(best_parameters.keys()):
    print("%s: %r" % (param_name, best_parameters[param_name]))

y_pred = grid_clf.predict(X_test2)

cnf_matrix = confusion_matrix(y_test, y_pred)
plot_confusion_matrix(cnf_matrix, classes=[0,1])

perf(y_test, y_pred)

plot_feature_importances(model,X_train)

"""#### **6. Conclusion**

1. The best result is given by Support Vector Machine model, which gives us 65% of loans that won't be repaid. However, the lender will only fund 36% of the loans that will be paid off.
2. The most important features in determining qualified borrowers are Amount of Open Credit Lines, Interest Rate of Loan, and Employment Length

#### **7. Next Steps**

1. Optimize Random Forest Hyperparameters to Improve Model
2. Work on Class Imbalance to Improve Predicting Power of the Model
3. Determine Levels of Interest Rates Insuring Profitability Despites Undetected Defaults
"""